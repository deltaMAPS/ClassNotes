{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b> Disclaimer: </b> Visualizations and presentation of theory is heavily based on the excellent book by Yoav Goldberg \"Neural network methods for natural language processing.\" Synthesis Lectures on Human Language Technologies 10.1 (2017) </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, LSTM, Bidirectional, TimeDistributed, Dense\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import to_categorical\n",
    "from keras import callbacks\n",
    "\n",
    "import ner_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Recurrent Neural Netowk (RNN) Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example, forward pass of an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##input dimension d_x\n",
    "input_dim = 10;\n",
    "##output dimension d_s\n",
    "output_dim = 5;\n",
    "\n",
    "print('Input dimension: '+str(input_dim))\n",
    "print('Output dimension: '+str(output_dim))\n",
    "\n",
    "##input (sequence) lebgth\n",
    "t = 100;\n",
    "##input initialization\n",
    "X = np.random.rand(input_dim,t);\n",
    "print('Input shape: '+str(X.shape));\n",
    "\n",
    "##state with dimension (1,d_s)\n",
    "##set it to 0 (s_0) initially\n",
    "s_i = np.expand_dims(np.zeros([output_dim]),0);\n",
    "##and output at time t will also be an empty (for now) matrix with dimensions [output_dim,t]\n",
    "y = np.zeros([output_dim,t]);\n",
    "print(y.shape)\n",
    "\n",
    "##lets initialize weights and biases\n",
    "##state weight vector of dimension d_s x d_s\n",
    "W_s = np.random.rand(output_dim, output_dim);\n",
    "##bias vector of dimension d_s\n",
    "b = np.random.rand(output_dim);\n",
    "##input weight vector of dimension d_x x d_s\n",
    "W_x = np.random.rand(input_dim,output_dim);\n",
    "\n",
    "\n",
    "print('Weight and bias dimensions');\n",
    "print('W_s '+str(W_s.shape));\n",
    "print('b '+str(b.shape));\n",
    "print('W_x '+str(W_x.shape))\n",
    "\n",
    "##forward pass\n",
    "for i in np.arange(0,t):\n",
    "    ##compute g( s_{i-1}W^s + x_iW^x +b ) which will give you the new state, here g will be tanh\n",
    "    ##get current input and put it in a shape of (1,dx)\n",
    "    x_i = np.expand_dims(X[:,i],0);\n",
    "    ##compute next state\n",
    "    s_i_next = np.tanh(np.matmul(s_i,W_s) + np.matmul(x_i,W_x) + b);\n",
    "    ##output\n",
    "    y[:,i] = s_i_next;\n",
    "    ##update state\n",
    "    s_i = s_i_next;\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Architectures (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example, forward pass of an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "##lets first quickly define the sigmoid function\n",
    "def sigmoid(x):    \n",
    "    return 1./(1. + np.exp(-x));\n",
    "\n",
    "##input dimension d_x\n",
    "input_dim = 100;\n",
    "##hidden dimension = output dimension = memmory cell dimension\n",
    "hidden_dim = 32;\n",
    "\n",
    "##input (sequence) lebgth\n",
    "t = 100;\n",
    "##input initialization, as before\n",
    "X = np.random.rand(input_dim,t);\n",
    "\n",
    "##initialize memmory cell and hidden cell each of dimension d_h\n",
    "h_i = np.expand_dims(np.zeros(hidden_dim),0);\n",
    "c_i = np.expand_dims(np.zeros(hidden_dim),0);\n",
    "\n",
    "##y output of dimension d_h x t\n",
    "y = np.zeros([hidden_dim,t]);\n",
    "\n",
    "\n",
    "##initialize weights and biases\n",
    "##similar to what we had for the RNN before, weights associated with input will have a\n",
    "##dimension of d_x x d_h, weights associated with hidden cells will have a dimension of d_h x d_h\n",
    "W_xi = np.random.rand(input_dim,hidden_dim) ##input gate weights for input\n",
    "W_hi = np.random.rand(hidden_dim,hidden_dim) ##input gate weights for hidden\n",
    "b_i = np.random.rand(hidden_dim); ##input gate bias \n",
    "\n",
    "W_xf = np.random.rand(input_dim,hidden_dim) ##forget weights for input\n",
    "W_hf = np.random.rand(hidden_dim,hidden_dim) ##forget weights for hidden\n",
    "b_f = np.random.rand(hidden_dim); ##forget bias \n",
    "\n",
    "W_xo = np.random.rand(input_dim,hidden_dim) ##output weights for input\n",
    "W_ho = np.random.rand(hidden_dim,hidden_dim) ##output weights for hidden\n",
    "b_o = np.random.rand(hidden_dim); ##output gate bias \n",
    "\n",
    "W_xz = np.random.rand(input_dim,hidden_dim) ##update candidate weights for input\n",
    "W_hz = np.random.rand(hidden_dim,hidden_dim) ##update candidate weights for hidden\n",
    "b_z = np.random.rand(hidden_dim); ##update candidate gate bias \n",
    "\n",
    "##and lets do a forward pass example\n",
    "for ii in np.arange(0,t):\n",
    "    ##get current input and put it in a shape of (1,dx)\n",
    "    x_i = np.expand_dims(X[:,ii],0);\n",
    "    \n",
    "    ##first we have the gatting mechanisms\n",
    "    i = sigmoid( np.matmul(x_i,W_xi) + np.matmul(h_i,W_hi) + b_i ); ##input gate\n",
    "    f = sigmoid( np.matmul(x_i,W_xf) + np.matmul(h_i,W_hf) + b_f ); ##forget gate\n",
    "    o = sigmoid( np.matmul(x_i,W_xo) + np.matmul(h_i,W_ho) + b_o ); ##output gate        \n",
    "    z = np.tanh( np.matmul(x_i,W_xz) + np.matmul(h_i,W_hz) + b_z); ##update candidate gate\n",
    "    \n",
    "    ##now we can compute the new memmory state\n",
    "    c_i_next = f*c_i+i*z;\n",
    "    ##and the next hidden state\n",
    "    h_i_next = o*np.tanh(c_i_next);\n",
    "    ##output and h_t are the same\n",
    "    y[:,ii] = h_i_next\n",
    "    ##update hidden and cell states   \n",
    "    h_i = h_i_next;\n",
    "    c_i = c_i_next;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many-to-one recurrent neural networks with Keras (Sentiment Detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> This section introduces variations of many-to-one recurrent neural networks with Keras. We start by loading a pre-processed example data set of imdb reviews. Sequences here are reviews and each sequence has a tag corresponding to a negative/positive review. Our goal is to use recurrent neural networks to perform sentiment analysis (aka predict the actual label of the sequence). </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many-to-one Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/many_to_one_simple.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Example of a many-to-one architecture </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing the imdb data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data\n",
    "<p>For a detailed description of the load_data function see: https://keras.io/datasets/ </p>\n",
    "\n",
    "<p>It is typical in many language tasks to exclude some words that are not frequent. These words will\n",
    "be substituted under the hood with an UNK (unknown) token (2 in our case) </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## This variable let us select the number of most frequent words to use, set to None to select everything\n",
    "max_num_words = 10000;\n",
    "print('Loading data...');\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_num_words);\n",
    "print('Done.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's look at how our data are represented\n",
    "print('Data types: ');\n",
    "print('Sentences: '+str(type(x_train)));\n",
    "print('Labels: '+str(type(y_train)));\n",
    "print('Data shape:');\n",
    "print('Sentences: '+str(x_train.shape)+' | labels: '+str(y_train.shape))\n",
    "\n",
    "for i in range(2):\n",
    "    print('Example data '+str(i));\n",
    "    print(x_train[i]);\n",
    "    print('Label: '+str(y_train[i]));\n",
    "    print('---');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> So our sentences are lists of integers, unknown words have been replaced with integer 2 while each sentence starts \n",
    "with integer 1 (the start of sentence symbol). In reality we always keep a dictionary that maps such integers to\n",
    "the actual words (and usually a reverse dictionary as well)</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess data\n",
    "<p>In the example above we observe that sentences have different length. In this first example architecture (many-to-one)\n",
    "    we are going to train our networks in <b> batches </b> of sentences/labels. That means that all sentences in a batch should have the same size. To achieve that we will pad sentences by either truncating them (if their length is less than the maximum\n",
    "desired length) or by adding a padding symbol until we reach the maximum desired length. Instead of doing this manually we will\n",
    "rely on a pre-built keras function pad_sequences (see here for details: https://keras.io/preprocessing/sequence/). </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Max sequence length\n",
    "max_len = 500;\n",
    "print('Shape before padding: '+str(x_train.shape));\n",
    "x_train = sequence.pad_sequences(x_train, maxlen = max_len);\n",
    "print('Number of unique words after padding: '+str(len(np.unique(x_train))));\n",
    "print('Shape after padding: '+str(x_train.shape)); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training many-to-one recurrent networks for sentiment detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> At this point we are ready to train and evaluate our models. We will illustrate and discuss several architectures. Simple RNNs, \n",
    "LSTMs, stacked LSTMs, bidirectional LSTMs and LSTMs that use dropout for <b> regularization </b>. All models use the same\n",
    "code structure to train (only the model definition changes + a few parameters) but we will repeat it for instructional purposes \n",
    "and just in case that you want to run a single model. </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We start by instantiating a Sequential keras model instance\n",
    "rnn_model = Sequential();\n",
    "## Now we add the Embedding vector, the embedding vector map integers to their corresponding tensors.\n",
    "## according to the documentation the input dimension is equal to the largest integer (i.e., word_index) +1\n",
    "## and this can get confusing... so do that and get over with it.\n",
    "edim = np.max(np.unique(x_train)) +1;\n",
    "## Next is our embedding dimension (32 in the following examples)\n",
    "## and mask_zeros = True deals with masking out the zeros\n",
    "rnn_model.add(Embedding(input_dim=edim,output_dim=32,mask_zero=True)); \n",
    "## Now add a simple recurrent network with 32 units\n",
    "rnn_model.add(SimpleRNN(32));   ##this is the only part that we will be changing from now on!\n",
    "## and on top add our dense layer and top it with a signoid activation\n",
    "rnn_model.add(Dense(1,activation='sigmoid'));\n",
    "\n",
    "## Final step is to compile everything together, when we compile the model we need to specify\n",
    "## what optimizer we are going to use, what loss are we going to use, and the metrics of interest\n",
    "## to report. \n",
    "rnn_model.compile(optimizer='rmsprop', loss = 'binary_crossentropy', metrics = ['acc']);\n",
    "\n",
    "## And we can also see what we did\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also visualize things if we install pydot and graphviz... \n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(rnn_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In these examples we will use the model fit function to train, the model fit function returns a \n",
    "## history object which is very useful when we want to visualize at the end our model's performance\n",
    "## we will deal with the visualization part at the end when we will compare the performance of all\n",
    "## recurrent neural networks. \n",
    "\n",
    "\n",
    "\n",
    "## For details and other options that you can use in the fit function see: https://keras.io/models/sequential/\n",
    "stime = time.time();\n",
    "rnn_history = rnn_model.fit(x_train, y_train,\n",
    "                       epochs = 10, ##number of epochs (passes through the data)\n",
    "                       batch_size = 128, ##batch size\n",
    "                       validation_split = 0.2, ##fraction of data to be used as validation\n",
    "                       shuffle = True ##shuffle data after each epoch\n",
    "                       );\n",
    "etime = time.time();\n",
    "print('Total time: '+str(etime-stime));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LSTM\n",
    "\n",
    "We follow exactly the same process as illustrated on the case of the RNN, the ONLY thing that changes\n",
    "is the type of the recurrent layer that we will use (and the fact that we now have fewer comments and\n",
    "do not plot summaries or architectures for the models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential();\n",
    "lstm_model.add(Embedding(input_dim=edim,output_dim=32,mask_zero=True)); \n",
    "\n",
    "##instead of a simple rnn now define a LSTM\n",
    "lstm_model.add(LSTM(32));   \n",
    "\n",
    "lstm_model.add(Dense(1,activation='sigmoid'));\n",
    "\n",
    "lstm_model.compile(optimizer='rmsprop', loss = 'binary_crossentropy', metrics = ['acc']);\n",
    "\n",
    "stime = time.time();\n",
    "lstm_history = lstm_model.fit(x_train, y_train,\n",
    "                       epochs = 10, ##number of epochs (passes through the data)\n",
    "                       batch_size = 128, ##batch size\n",
    "                       validation_split = 0.2, ##fraction of data to be used as validation\n",
    "                       shuffle = True ##shuffle data after each epoch\n",
    "                       );\n",
    "etime = time.time();\n",
    "print('Total time: '+str(etime-stime));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LSTM + dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/lstm_dropout.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Dropout in recurrent networks. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_lstm_model = Sequential();\n",
    "drop_lstm_model.add(Embedding(input_dim=edim,output_dim=32,mask_zero=True)); \n",
    "\n",
    "## Lets add some dropout to our basic LSTM. dropout here is to the input/output connection and recurrent to\n",
    "## the recurrent connection as we have illustrated in the figure above\n",
    "drop_lstm_model.add(LSTM(32,dropout=0.8, recurrent_dropout=0.75));   \n",
    "\n",
    "drop_lstm_model.add(Dense(1,activation='sigmoid'));\n",
    "\n",
    "drop_lstm_model.compile(optimizer='rmsprop', loss = 'binary_crossentropy', metrics = ['acc']);\n",
    "\n",
    "stime = time.time();\n",
    "drop_lstm_history = drop_lstm_model.fit(x_train, y_train,\n",
    "                       epochs = 10, ##number of epochs (passes through the data)\n",
    "                       batch_size = 128, ##batch size\n",
    "                       validation_split = 0.2, ##fraction of data to be used as validation\n",
    "                       shuffle = True ##shuffle data after each epoch\n",
    "                       );\n",
    "etime = time.time();\n",
    "print('Total time: '+str(etime-stime));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stacked LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![as](img/lstm_stacked.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Example of a stacked recurrent architecture. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>An interesting idea of using hierarchical architectures for sentiment detection is to have a first RNN decode sentences (or spans of words in a sentence) and a second one accepting such sentence representations. Intuition is that the first RNN learns sentiment of sentences while the second one learns sentiment of the document based on sentiment of sentences. See <i> Tang, Duyu, Bing Qin, and Ting Liu. \"Document modeling with gated recurrent neural network for sentiment classification.\" Proceedings of the 2015 conference on empirical methods in natural language processing. 2015.</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_lstm_model = Sequential();\n",
    "stacked_lstm_model.add(Embedding(input_dim=edim,output_dim=32,mask_zero=True)); \n",
    "\n",
    "## Instead of a LSTM we will define a stacked - hierarchical LSTM. The trick is that\n",
    "## the next LSTM layer needs input from the previous layer, so what we need to do\n",
    "## is to ask for the first layer to return its sequences. So by setting return sequences = True\n",
    "## we do not only return the last output of the input sequence but the full output\n",
    "stacked_lstm_model.add(LSTM(32, return_sequences = True));   \n",
    "## Lets add one more layer on top\n",
    "stacked_lstm_model.add(LSTM(16));\n",
    "\n",
    "\n",
    "stacked_lstm_model.add(Dense(1,activation='sigmoid'));\n",
    "\n",
    "stacked_lstm_model.compile(optimizer='rmsprop', loss = 'binary_crossentropy', metrics = ['acc']);\n",
    "\n",
    "stime = time.time();\n",
    "stacked_lstm_history = stacked_lstm_model.fit(x_train, y_train,\n",
    "                       epochs = 10, ##number of epochs (passes through the data)\n",
    "                       batch_size = 128, ##batch size\n",
    "                       validation_split = 0.2, ##fraction of data to be used as validation\n",
    "                       shuffle = True ##shuffle data after each epoch\n",
    "                       );\n",
    "etime = time.time();\n",
    "print('Total time: '+str(etime-stime));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/lstm_bidirectional.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Example architecture of a bidirectional recurrent architecture. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_model = Sequential();\n",
    "bilstm_model.add(Embedding(input_dim=edim,output_dim=32,mask_zero=True)); \n",
    "\n",
    "## Instead of a simple rnn now define a LSTM\n",
    "## bi-directional is a wrapper layer here that hides from us the fact that it\n",
    "## runs separately two LSTMs and then concats the output\n",
    "bilstm_model.add(Bidirectional(LSTM(32)));   \n",
    "\n",
    "bilstm_model.add(Dense(1,activation='sigmoid'));\n",
    "\n",
    "bilstm_model.compile(optimizer='rmsprop', loss = 'binary_crossentropy', metrics = ['acc']);\n",
    "\n",
    "\n",
    "\n",
    "stime = time.time();\n",
    "bilstm_history = bilstm_model.fit(x_train, y_train,\n",
    "                       epochs = 10, ##number of epochs (passes through the data)\n",
    "                       batch_size = 128, ##batch size\n",
    "                       validation_split = 0.2, ##fraction of data to be used as validation\n",
    "                       shuffle = True ##shuffle data after each epoch\n",
    "                       );\n",
    "etime = time.time();\n",
    "print('Total time: '+str(etime-stime));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results - Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,8]);\n",
    "\n",
    "## First we will plot the train and validation(test) accuracy at each epoch.\n",
    "## The history returned from the fit(...) method has everything we need!\n",
    "plt.plot(rnn_history.history['acc'],'-bo',linewidth=2,label='RNN Train');\n",
    "plt.plot(lstm_history.history['acc'],'-ro',linewidth=2,label='LSTM Train');\n",
    "plt.plot(drop_lstm_history.history['acc'],'-go',linewidth=2,label='LSTM + dropout Train');\n",
    "plt.plot(stacked_lstm_history.history['acc'],'-mo',linewidth=2,label='Stacked LSTM Train');\n",
    "plt.plot(bilstm_history.history['acc'],'-ko',linewidth=2,label='BiLSTM Train');\n",
    "\n",
    "plt.plot(rnn_history.history['val_acc'],':bo',linewidth=2,label='RNN Test');\n",
    "plt.plot(lstm_history.history['val_acc'],':ro',linewidth=2,label='LSTM Test');\n",
    "plt.plot(drop_lstm_history.history['val_acc'],':go',linewidth=2,label='LSTM + dropout Test');\n",
    "plt.plot(stacked_lstm_history.history['val_acc'],':mo',linewidth=2,label='Stacked LSTM Test');\n",
    "plt.plot(bilstm_history.history['val_acc'],':ko',linewidth=2,label='BiLSTM Test');\n",
    "\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylabel('Accuracy');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,8]);\n",
    "## Then we will plot the train and validation(test) loss at each epoch.\n",
    "plt.plot(rnn_history.history['loss'],'-bo',linewidth=2,label='RNN Train');\n",
    "plt.plot(lstm_history.history['loss'],'-ro',linewidth=2,label='LSTM Train');\n",
    "plt.plot(drop_lstm_history.history['loss'],'-go',linewidth=2,label='LSTM + dropout Train');\n",
    "plt.plot(stacked_lstm_history.history['loss'],'-mo',linewidth=2,label='Stacked LSTM Train');\n",
    "plt.plot(bilstm_history.history['loss'],'-ko',linewidth=2,label='BiLSTM Train');\n",
    "\n",
    "plt.plot(rnn_history.history['val_loss'],':bo',linewidth=2,label='RNN Test');\n",
    "plt.plot(lstm_history.history['val_loss'],':ro',linewidth=2,label='LSTM Test');\n",
    "plt.plot(drop_lstm_history.history['val_loss'],':go',linewidth=2,label='LSTM + dropout Test');\n",
    "plt.plot(stacked_lstm_history.history['val_loss'],':mo',linewidth=2,label='Stacked LSTM Test');\n",
    "plt.plot(bilstm_history.history['val_loss'],':ko',linewidth=2,label='BiLSTM Test');\n",
    "\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylabel('Loss');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many-to-many recurrent neural netwokrs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> In this section we will deal with the problem of <b> named entity recognition </b>. As the name suggests the problem is, given a sentence (e.g., John lives in New York and works at Facebook) to identify the (named) entities in that sentence (John, New York, Facebook) as well as the entity type (Person, Location, Organization).  </p>\n",
    "\n",
    "<p> It is easy to understand that in this problem our input to the recurrent network will be a sequence and the output will also be a sequence (of named entities) as well - a \"many-to-many\" relationship. </p>\n",
    "\n",
    "<p> Here is an example of a tagged sentence from our data: </p>\n",
    "\n",
    "EU  B-ORG <br/>\n",
    "rejects  O <br/>\n",
    "German  B-MISC <br/>\n",
    "call  O <br/>\n",
    "to  O <br/>\n",
    "boycott  O <br/>\n",
    "British  B-MISC <br/>\n",
    "lamb   O <br/>\n",
    ".  O <br/>\n",
    "\n",
    "<p> The tags of the dataset are MISC,PER,ORG,LOC and O corresponding to miscellaneous, person, organization, location and \"other\" respectively. Each tag has a prefix B/I which determines the order of the named entity (e.g., New York -> B-LOC I-LOC) </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/many_to_many_simple.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Many-to-one architecture example.  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and parsing the data\n",
    "\n",
    "To keep things cleaner, and for time's shake we will use the <b> ner_data_loader.py </b> script to load the data. The script is well documented and you can refer to it after the end of the class to see the details. Functions in there are pretty trivial and deal mostly with parsing files and getting everything to a shape similar to what we get from the imdb.load_data() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing data\n",
      "Done.\n",
      "Sentences: 11373\n",
      "Max sentence length: 62\n",
      "Min sentence length: 5\n",
      "Unique words on corpus: 23624\n",
      "Unique labels on corpus: 9\n",
      "Pruning non-frequent words, keeping top 20000\n",
      "Unique words in data: 19522\n",
      "Unique labels in data: 9\n"
     ]
    }
   ],
   "source": [
    "fname = \"train.txt\"; #filename holding the data, should be at the same folder as this notebook, else specify a full path\n",
    "\n",
    "##step 1. Load the raw data, keep sentences with length ranging from [min_length, max_length]\n",
    "sentences, sentence_labels = ner_data_loader.load_data(fname,min_length=5,max_length=64,)\n",
    "##step 2. construct dictionaries mapping words and labels to integer indices and vice versa\n",
    "word_to_ind, ind_to_word, label_to_ind, ind_to_label = ner_data_loader.get_dictionaries(sentences, sentence_labels);\n",
    "##step 3. Bring the data to the same format (list of lists) that we used in the imdb.load_data() function.\n",
    "## here we do not use a start symbol.\n",
    "sentences_ind = ner_data_loader.to_index(sentences,word_to_ind);\n",
    "sentence_labels_ind = ner_data_loader.to_index(sentence_labels, label_to_ind);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence example: \n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Labels for sentence\n",
      "[1, 2, 3, 2, 2, 2, 3, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print('Sentence example: ');\n",
    "print(sentences_ind[0]);\n",
    "print('Labels for sentence');\n",
    "print(sentence_labels_ind[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to what we did before we will start by padding the sentences to the same length. \n",
    "Since now the labels are not 0/1 but have the same length as the corresponding sentence, \n",
    "labels will need to be padded as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 64;  ##max sentence length\n",
    "x_train = sequence.pad_sequences(sentences_ind, maxlen=max_len);\n",
    "y_train = sequence.pad_sequences(sentence_labels_ind, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dealing with categorical data and not with a binary 0/1 example. So what we need to do is to\n",
    "one-hot encode the binary labels in sentence_labels_ind to one hot-vectors. We will use the function\n",
    "below to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function that one-hot encodes the integer labels. \n",
    "\n",
    "Inputs:\n",
    "    y_train (nd-array): An array of size [samples, max_len] holding the padded labels of each sentence\n",
    "Returns:\n",
    "    y_train_onehot (nd-array): An array of size [samples, max_len, one_hot_vector_size]. The one hot vector\n",
    "    length is 1 + the original labels (the +1 comes for the padding symbol)\n",
    "\"\"\"\n",
    "def one_hot_encode(y_train):\n",
    "    ##dimensions ndata x time_ x labels+1\n",
    "    n_labels_p1 = len(np.unique(y_train)); ##number of labels +1 since y_train is padded\n",
    "    y_train_onehot = np.zeros([y_train.shape[0], y_train.shape[1], n_labels_p1]); \n",
    "    \n",
    "    for i in np.arange(y_train.shape[0]):\n",
    "        for j in np.arange(y_train.shape[1]):\n",
    "            y_train_onehot[i][j] = to_categorical(y_train[i][j], n_labels_p1);\n",
    "    return y_train_onehot;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before... (11373, 64)\n",
      "Shape after... (11373, 64, 10)\n",
      "Example 1-hot label for the first word of the first sentence\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "##and let's one hot encode...\n",
    "print('Shape before... '+str(y_train.shape));\n",
    "y_train = one_hot_encode(y_train);\n",
    "print('Shape after... '+str(y_train.shape));\n",
    "print('Example 1-hot label for the first word of the first sentence')\n",
    "print(y_train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and compiling the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##nothing to see here same as before\n",
    "embedding_input_dim = np.max(np.unique(x_train))+1;\n",
    "embedding_dim = 128;\n",
    "hidden_size = 64;\n",
    "mlp_size = 32;\n",
    "n_outputs = len(label_to_ind)+1;\n",
    "batch_size = 64;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 64, 128)           2498944   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64, 128)           98816     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 64, 32)            4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64, 10)            330       \n",
      "=================================================================\n",
      "Total params: 2,602,218\n",
      "Trainable params: 2,602,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Here's the main model.\n",
    "ner_model = Sequential();\n",
    "## Important part number one, we will need to use the TimeDistributed WRAPPER. That wrapper applies a\n",
    "## layer to each (temporal) part of the sequence (we'll discuss it more in class).\n",
    "## So, we need to use set input_length argument, which signifies that all sequences will have\n",
    "## a fixed size of max_len (the maximum sentence length defined earlier)\n",
    "ner_model.add(Embedding(input_dim=embedding_input_dim, \n",
    "                        output_dim=embedding_dim, \n",
    "                        mask_zero=False,\n",
    "                        input_length = max_len));\n",
    "\n",
    "## Important part number two, our LSTMs NEEEEED to return the sequences so we can apply a layer/function\n",
    "## to all the parts of the sequence. \n",
    "\n",
    "ner_model.add(Bidirectional(\n",
    "            LSTM(hidden_size, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)\n",
    "          ));\n",
    "\n",
    "\n",
    "# ner_model.add(SimpleRNN(2, return_sequences=True));\n",
    "\n",
    "# #ner_model.add(LSTM(128, return_sequences=True));\n",
    "\n",
    "\n",
    "## We use softmax since n_classes > 1 to get a pseudo-probability for each class\n",
    "ner_model.add(TimeDistributed(Dense(mlp_size)));\n",
    "ner_model.add(Dense(n_outputs, activation='softmax'));\n",
    "\n",
    "#ner_model.add(TimeDistributed(Dense(n_outputs, activation='softmax')));\n",
    "\n",
    "## We also need to use categorical crossentropy (which implies > 1 classes with 1 hot vectors)\n",
    "ner_model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop',metrics=['acc']);\n",
    "\n",
    "## And, we ready to roll\n",
    "ner_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9098 samples, validate on 2275 samples\n",
      "Epoch 1/10\n",
      "9098/9098 [==============================] - 108s 12ms/step - loss: 0.1929 - acc: 0.9503 - val_loss: 0.1014 - val_acc: 0.9684\n",
      "Epoch 2/10\n",
      "9098/9098 [==============================] - 105s 11ms/step - loss: 0.0677 - acc: 0.9796 - val_loss: 0.0701 - val_acc: 0.9803\n",
      "Epoch 3/10\n",
      "9098/9098 [==============================] - 103s 11ms/step - loss: 0.0363 - acc: 0.9899 - val_loss: 0.0549 - val_acc: 0.9844\n",
      "Epoch 4/10\n",
      "9098/9098 [==============================] - 104s 11ms/step - loss: 0.0229 - acc: 0.9938 - val_loss: 0.0480 - val_acc: 0.9859\n",
      "Epoch 5/10\n",
      "9098/9098 [==============================] - 105s 12ms/step - loss: 0.0165 - acc: 0.9956 - val_loss: 0.0454 - val_acc: 0.9871\n",
      "Epoch 6/10\n",
      "9098/9098 [==============================] - 104s 11ms/step - loss: 0.0129 - acc: 0.9965 - val_loss: 0.0484 - val_acc: 0.9854\n",
      "Epoch 7/10\n",
      "9098/9098 [==============================] - 104s 11ms/step - loss: 0.0109 - acc: 0.9972 - val_loss: 0.0452 - val_acc: 0.9878\n",
      "Epoch 8/10\n",
      "9098/9098 [==============================] - 104s 11ms/step - loss: 0.0091 - acc: 0.9976 - val_loss: 0.0429 - val_acc: 0.9883\n",
      "Epoch 9/10\n",
      "9098/9098 [==============================] - 104s 11ms/step - loss: 0.0078 - acc: 0.9979 - val_loss: 0.0452 - val_acc: 0.9871\n",
      "Epoch 10/10\n",
      "9098/9098 [==============================] - 103s 11ms/step - loss: 0.0070 - acc: 0.9982 - val_loss: 0.0451 - val_acc: 0.9882\n"
     ]
    }
   ],
   "source": [
    "ner_model.fit(x_train, y_train, epochs=10, batch_size=32, \n",
    "              validation_split=0.2,shuffle=True,\n",
    "              callbacks = [callbacks.TensorBoard(log_dir=\"./logs\", histogram_freq=1, write_grads=True)]\n",
    "             );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start tensorboard just run the command <b> tensorboard --logdir /path/to/log0directory </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's predict something \n",
    "(to prove to your parents/boss/significant other that they spent that GPU money wisely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentence we are trying to identify named entities forb\n",
    "sentence = \"My name is John and I work at Microsoft Corp\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we follow the same paradigm and we transform the sentence to a padded sequence\n",
    "\n",
    "split_sentence = sentence.split(' ');\n",
    "indexed_sentence = [];\n",
    "for word in split_sentence:\n",
    "    if(not word in word_to_ind):\n",
    "        indexed_sentence.append(word_to_ind[\"UNK\"]);\n",
    "    else:\n",
    "        indexed_sentence.append(word_to_ind[word])\n",
    "\n",
    "##remember, we need to add the batch dimension    \n",
    "indexed_sentence = np.expand_dims(indexed_sentence,0);\n",
    "##and our model is expecting a sentence of length 64 so pad...\n",
    "indexed_sentence = sequence.pad_sequences(indexed_sentence,maxlen=max_len);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## and the results...\n",
    "pred_classes = ner_model.predict_classes(indexed_sentence);\n",
    "pred_classes = pred_classes.flatten();\n",
    "indexed_sentence = indexed_sentence.flatten();\n",
    "\n",
    "for i in range(len(pred_classes)):\n",
    "    try:\n",
    "        print(ind_to_word[indexed_sentence[i]]+'\\t'+ind_to_label[pred_classes[i]])\n",
    "    except KeyError:\n",
    "        pass;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the end (Sometimes life is unfair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##suppose that you want to add pre-trained Embeddings to an LSTM or whatever...\n",
    "##here's a \"pre-trained\" vector\n",
    "embeddings = np.random.rand(5,8); ##vocabulary is 5 words and embeddings have 8 dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##how to add the embeddings???\n",
    "##let's read the documentation (https://keras.io/layers/embeddings/)\n",
    "##nothing there... \n",
    "## but embedding is an instance of the engine.base_layer.Layer and there\n",
    "## you have options such as trainable and weights = ... \n",
    "## I can not find anything on the documentation though (so there might be another\n",
    "## way to manually set weights and freeze a layer but @!### off I'm stupid ok?)\n",
    "\n",
    "##in any way let's move on\n",
    "\n",
    "##simple model...\n",
    "model = Sequential();\n",
    "##add Embedding layer 5 inputs 8 outputs, tell keras not to train it and then set your own embeddings\n",
    "model.add(Embedding(input_dim=5,output_dim=8,trainable=False,weights=embeddings));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now google that error and good luck\n",
    "##do not forget to feel bad about yourself...\n",
    "##somehow, somewhere I got the following \"brilliant\" idea, because sometimes people are just evil\n",
    "model.add(Embedding(input_dim=5,output_dim=8,trainable=False,weights=[embeddings]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##yes...\n",
    "##aha...\n",
    "##mhhhmmm...\n",
    "## from the author of keras: https://blog.keras.io/user-experience-design-for-apis.html\n",
    "## but ok, it is a great library to do stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
